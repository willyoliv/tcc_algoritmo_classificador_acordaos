{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b415ef4",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71703c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Acordao:\n",
    "    def __init__(self, ementa, assunto):\n",
    "        self.ementa = ementa\n",
    "        self.assunto = assunto\n",
    "        \n",
    "class AcordaoContainer:\n",
    "    def __init__(self, acordaos):\n",
    "        self.acordaos = acordaos\n",
    "    \n",
    "    def get_x(self, vectorizer):\n",
    "        return vectorizer.transform(self.get_ementa())\n",
    "    \n",
    "    def get_ementa(self):\n",
    "        return [x.ementa for x in self.acordaos]\n",
    "    \n",
    "    def get_assunto(self):\n",
    "        return [x.assunto for x in self.acordaos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a846c813",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8950525d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, recall_score, precision_score, accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25648bf3",
   "metadata": {},
   "source": [
    "### Carregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c077d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESPESA                104\n",
      "PROCESSUAL              99\n",
      "LICITACAO               97\n",
      "PESSOAL                 94\n",
      "OUTROS                  84\n",
      "PRESTACAO DE CONTAS     72\n",
      "CONTRATO                68\n",
      "PREVIDENCIA             56\n",
      "AGENTE POLITICO         49\n",
      "RESPONSABILIDADE        38\n",
      "Name: assunto_agrupado, dtype: int64\n",
      "DESPESA                26\n",
      "LICITACAO              25\n",
      "PROCESSUAL             25\n",
      "PESSOAL                24\n",
      "OUTROS                 22\n",
      "PRESTACAO DE CONTAS    19\n",
      "CONTRATO               18\n",
      "PREVIDENCIA            14\n",
      "AGENTE POLITICO        13\n",
      "RESPONSABILIDADE       10\n",
      "Name: assunto_agrupado, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dados pra treino\n",
    "acordaosTrain = []\n",
    "ementasTrain = []\n",
    "assuntosTrain = []\n",
    "dataTrain = pd.read_csv (r'C:\\Users\\willy\\Desktop\\TCC_classificador\\Dados\\decisoes_com_dados_adicionais_train.csv', sep=\";\")   \n",
    "dfTrain = pd.DataFrame(dataTrain, columns= ['ementa','assunto_agrupado'])\n",
    "ementasTrain = dfTrain['ementa']\n",
    "assuntosTrain = dfTrain['assunto_agrupado']\n",
    "print(dfTrain['assunto_agrupado'].value_counts())\n",
    "\n",
    "# Dados pra teste\n",
    "acordaosTest = []\n",
    "ementasTest = []\n",
    "assuntosTest = []\n",
    "dataTest = pd.read_csv (r'C:\\Users\\willy\\Desktop\\TCC_classificador\\Dados\\decisoes_com_dados_adicionais_test.csv', sep=\";\")   \n",
    "dfTest = pd.DataFrame(dataTest, columns= ['ementa','assunto_agrupado'])\n",
    "ementasTest = dfTest['ementa']\n",
    "assuntosTest = dfTest['assunto_agrupado'].apply(lambda x: np.str_(x))\n",
    "print(dfTest['assunto_agrupado'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a4d286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(ementasTrain)):\n",
    "    acordaosTrain.append(Acordao(ementasTrain[i],assuntosTrain[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5d4b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(ementasTest)):\n",
    "    acordaosTest.append(Acordao(ementasTest[i],assuntosTest[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98655c86",
   "metadata": {},
   "source": [
    "### Separando os dados para treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0d63cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_container = AcordaoContainer(acordaosTrain)\n",
    "test_container = AcordaoContainer(acordaosTest)\n",
    "\n",
    "corpus = train_container.get_ementa()\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7)\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "#array para treinamento do modelo \n",
    "train_x = train_container.get_x(vectorizer)\n",
    "train_y = train_container.get_assunto()\n",
    "#array para teste do modelo\n",
    "test_x = test_container.get_x(vectorizer)\n",
    "test_y = test_container.get_assunto()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994be86",
   "metadata": {},
   "source": [
    "### Criando modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "097eef54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM SVC\n",
    "svm_model = svm.SVC(C=16, kernel='linear', gamma='auto')\n",
    "svm_model.fit(train_x, train_y)\n",
    "\n",
    "# Naive Bayes\n",
    "nv_model = MultinomialNB()\n",
    "nv_model.fit(train_x, train_y)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_model.fit(train_x, train_y) \n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a71920eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valor real LICITACAO\n",
      "predição ['DESPESA']\n",
      "valor real LICITACAO\n",
      "predição ['LICITACAO']\n",
      "valor real LICITACAO\n",
      "predição ['PESSOAL']\n",
      "valor real LICITACAO\n",
      "predição ['PESSOAL']\n"
     ]
    }
   ],
   "source": [
    "print(\"valor real \"+test_y[1])\n",
    "print(\"predição\",svm_model.predict(test_x[1]))\n",
    "print(\"valor real \"+test_y[1])\n",
    "print(\"predição\", nv_model.predict(test_x[1]))\n",
    "print(\"valor real \"+test_y[1])\n",
    "print(\"predição\",rf_model.predict(test_x[1]))\n",
    "print(\"valor real \"+test_y[1])\n",
    "print(\"predição\", lr_model.predict(test_x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80996d7",
   "metadata": {},
   "source": [
    "### Testando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a40a95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "F1 score: 0.8609413563517967\n",
      "Recall score: 0.8622448979591837\n",
      "Precision score: 0.8621620726212563\n",
      "Accuracy score: 0.8622448979591837 \n",
      "\n",
      "Naive Bayes\n",
      "F1 score: 0.6408830437946192\n",
      "Recall score: 0.6632653061224489\n",
      "Precision score: 0.6911992770621491\n",
      "Accuracy score: 0.6632653061224489 \n",
      "\n",
      "Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.7970694228825992\n",
      "Recall score: 0.8061224489795918\n",
      "Precision score: 0.8050954409663883\n",
      "Accuracy score: 0.8061224489795918 \n",
      "\n",
      "Logistic Regression\n",
      "F1 score: 0.7429410240467642\n",
      "Recall score: 0.7551020408163265\n",
      "Precision score: 0.7818392031721113\n",
      "Accuracy score: 0.7551020408163265 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "print(\"SVM\")\n",
    "svm_predict = svm_model.predict(test_x)\n",
    "print(\"F1 score:\",f1_score(test_y, svm_predict, average='weighted'))\n",
    "print(\"Recall score:\",recall_score(test_y, svm_predict, average='weighted'))\n",
    "print(\"Precision score:\",precision_score(test_y, svm_predict, average='weighted'))\n",
    "print(\"Accuracy score:\",accuracy_score(test_y, svm_predict, normalize = True),'\\n')\n",
    "\n",
    "# Naive Bayers\n",
    "print(\"Naive Bayes\")\n",
    "nv_predict = nv_model.predict(test_x)\n",
    "print(\"F1 score:\",f1_score(test_y, nv_predict, average='weighted'))\n",
    "print(\"Recall score:\",recall_score(test_y, nv_predict, average='weighted'))\n",
    "print(\"Precision score:\",precision_score(test_y, nv_predict, average='weighted'))\n",
    "print(\"Accuracy score:\",accuracy_score(test_y, nv_predict, normalize = True),'\\n')\n",
    "\n",
    "# Random Forest\n",
    "print(\"Random Forest\")\n",
    "rf_predict = rf_model.predict(test_x)\n",
    "print(\"F1 score:\",f1_score(test_y, rf_predict, average='weighted'))\n",
    "print(\"Recall score:\",recall_score(test_y, rf_predict, average='weighted'))\n",
    "print(\"Precision score:\",precision_score(test_y, rf_predict, average='weighted'))\n",
    "print(\"Accuracy score:\",accuracy_score(test_y, rf_predict, normalize = True),'\\n')\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"Logistic Regression\")\n",
    "lr_predict = lr_model.predict(test_x)\n",
    "print(\"F1 score:\",f1_score(test_y, lr_predict, average='weighted'))\n",
    "print(\"Recall score:\",recall_score(test_y, lr_predict, average='weighted'))\n",
    "print(\"Precision score:\",precision_score(test_y, lr_predict, average='weighted'))\n",
    "print(\"Accuracy score:\",accuracy_score(test_y, lr_predict, normalize = True),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6327982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
